{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Part 0: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I just explore the data to understand better its properties and distribution, experiment with some ideas for text pre-processing, and use a number of \"classic\" ML methods to obtain a baseline for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SIZE: (14640, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0                @VirginAmerica What @dhepburn said.   neutral\n",
       "1  @VirginAmerica plus you've added commercials t...  positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...   neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...  negative\n",
       "4  @VirginAmerica and it's a really big bad thing...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from the CSV file\n",
    "INPUT_PATH = 'twitter-airline-sentiment/Tweets.csv'\n",
    "raw_data = pd.read_csv(INPUT_PATH, header=0)\n",
    "\n",
    "df = raw_data.copy()[['text', 'airline_sentiment']]\n",
    "df = df.rename(columns={'airline_sentiment': 'sentiment'})\n",
    "\n",
    "print(\"DATA SIZE: \" + str(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "negative   9178\n",
       "neutral    3099\n",
       "positive   2363"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset has 14640 tweets, of which the big majority (9178) are negative, 2363 are positive and there are some neutrals. We also see that are many Twitter handlers, which normally refer to entities. Let's check them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of handlers = 889, with more than 1 occurrence = 193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@united', 3893),\n",
       " ('@usairways', 2998),\n",
       " ('@americanair', 2961),\n",
       " ('@southwestair', 2458),\n",
       " ('@jetblue', 2248),\n",
       " ('@virginamerica', 518),\n",
       " ('@delta', 68),\n",
       " ('@imaginedragons', 45),\n",
       " ('@phlairport', 20),\n",
       " ('@dfwairport', 17),\n",
       " ('@wsj', 13),\n",
       " ('@ladygaga', 12),\n",
       " ('@carrieunderwood', 12),\n",
       " ('@fortunemagazine', 12),\n",
       " ('@love_dragonss', 10),\n",
       " ('@virginatlantic', 9),\n",
       " ('@flytpa', 9),\n",
       " ('@cowboycerrone', 9),\n",
       " ('@staralliance', 8),\n",
       " ('@gg8929', 8)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find the handlers\n",
    "from collections import Counter\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "def get_text_sequence(_df):\n",
    "    return (row.text for _, row in _df.iterrows())\n",
    "\n",
    "def get_all_handlers(text_it):\n",
    "    handler_counts = Counter()\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    \n",
    "    for text in text_it:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        handler_counts.update(t for t in tokens if len(t) > 1 and t.startswith('@'))\n",
    "\n",
    "    return handler_counts\n",
    "\n",
    "handler_counts = get_all_handlers(get_text_sequence(df))\n",
    "num_handlers_more_1 = sum(1 if c > 1 else 0 for c in handler_counts.values())\n",
    "print(\"Number of handlers = %d, with more than 1 occurrence = %d\" % (len(handler_counts), num_handlers_more_1))\n",
    "handler_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge majority of handlers refer to the airlines themselves, also considering that every tweet starts with one of them.\n",
    "\n",
    "Speaking of it, let's check the distribution of sentiment between airlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2707e46d8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIwCAYAAAB5toZ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xu4VWW99//3V0DxhKhhqaig2xQQXMpBy+2WsvBUOyuPWaGW6E9tW0/ZJnemT4edj9vynHbQtCLxnO4n984ySks0wVBD9PG0FISNqIl4IAG/vz/GWLTEBesGFmuuBe/Xdc1rzXmPMe75nXNafOY973GPyEwkSZIktW+9RhcgSZIkdReGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKmQ4VnSOiMiLo+IMxtdh1ZPRJwRET9awfYBEZER0bN+/F8RMbbzKpS0NgvXeZbUSBHxj8C5wBBgCTAD+Hxm3rea/R4LfDYz/3G1i1xNEXE28A+Z+clG19JaRDRTvUe/aXQtHSkiBgBPAb0yc3Fjq5G0tunZ6AIkrbsiog/wf4H/D7gOWB/YF/hbI+tS54mInh0ZcFtGmyVpTXHahqRGejdAZl6TmUsy8/XMvD0zH2zZISKOj4gZEfHXiPhVROzQaltGxEkR8Vi9/dKoDAIuB94TEa9ExEv1/ldFxDfr+6MjYlZEfDkinouIORFxaEQcHBH/LyJejIgzWj3XehExPiKeiIgXIuK6iNii3tYyTWBsRDwTEc9HxL/V2w4EzgCOrGt5oK03IiK2i4ibImJe3f8lrZ73qxHxdF3nTyJis9avYZl+miPiA/X9s+s6fxIRCyJiekSMqLf9FNge+M+6ri9HRO+I+Fn9/C9FxH0R8c7l1NscEV+JiIfr9/7HEdG71fYPRcS0up+7I2LYMsf+a0Q8CLzaVuCNiAsjYmZEvBwRUyNi31bbzo6Iny3z3n8mIp4BfttGX7+LiM/W94+NiD9ExHl13U9FxEGt9t0sIq6o/3t4NiK+GRE92noPJK2bDM+SGun/AUsi4uqIOCgiNm+9MSIOpQqeHwP6AXcB1yzTx4eAkcDuwBHAAZk5AzgJmJyZm2Rm3+U8/7uA3sC2wNeAHwKfBIZTjYB/LSJ2rPf9F+BQYD9gG+CvwKXL9PePwC7A/vWxgzLzv4F/B66ta9l92SLqcPZ/gaeBAXU9E+vNx9a39wE7ApsAlyzn9bTln+u++gK3thybmZ8CngE+XNd1LjAW2AzYDtiS6j18fQV9HwMcAOxE9UXoq/Xr2RO4Ejix7uf7wK0RsUGrY48GDgH6Lmfk+T6gCdgC+Dlwfetw3ob9gEF1Pe3ZC3gUeAfVlKErIiLqbVcDi4F/APYAxgCfLehT0jrC8CypYTLzZarAmVTBdV5E3NpqtPNE4NuZOaMOWP8ONLUefQbOycyXMvMZYBJV4Cq1CPhWZi6iCpjvAC7MzAWZOR2YDrSMmJ4I/FtmzsrMvwFnA4ctM2r6v+vR8weAB6gCfYlRVIH89Mx8NTMXZuYf6m3HAN/NzCcz8xXgK8BRbY3WLscfMvO2zFwC/LSdmhZRhd1/qH8JmFp/RstzSWbOzMwXgW9RBWKAE4DvZ+a9dT9XU03F2bvVsRfVx7YZzjPzZ5n5QmYuzszvABtQfTFZnrPr925FYb/F05n5w/o9uRrYGnhn/d/dQVRz7l/NzOeA84GjCvqUtI4wPEtqqDoYH5uZ/YHdqELkBfXmHYAL65/+XwJeBIJqZLbF/7S6/xrVyGypF+oABX8fYZ3bavvrrfrbAbi5VS0zqE5wbD2tYVVr2Y4q0LU1ArsN1Yh0i6epzldpczpFG5atqfcKgvdPgV8BEyNidkScGxG9VtD3zGXq2qa+vwPwxZb3qn6/tmu1fdlj3yYivhjVdJ359fGbUX25KamlPUvfk8x8rb67SV13L2BOq7q/D2y1En1LWssZniV1GZn5CHAVVYiGKhCdmJl9W902zMy7S7rr4PJmAgctU0vvzHy2A2qZCWy/nFA7myrUtdiealrBXOBVYKOWDfX0j34F9bRZV2Yuysz/nZmDgfdSTYn59AqO326ZumbX92dSjei3fq82yszWU26W+57U85v/lWoazub1tJv5VF+cil7LKppJNUL+jlZ198nMIR3Qt6S1hOFZUsNExK71CGP/+vF2VD/931PvcjnwlYgYUm/fLCIOL+x+LtA/ItbvoHIvB77VMmUkIvpFxEdWopYBEbG8/8/9EzAHOCciNq5P3Nun3nYN8IWIGBgRm/D3+dOLqeaM946IQ+oR4q9STW8oNZdqHjX1a3pfRAytQ/jLVNM4lizvYOCUiOgf1YmTZwDX1u0/BE6KiL2isnFd46aFdW1K9QVhHtAzIr4G9FmJ17VKMnMOcDvwnYjoE9XJmjtFxH5r+rkldR+GZ0mNtIDq5K17I+JVqtD8F+CLAJl5M/B/qKYRvFxvO2g5fS3rt1Rzlv8nIp7vgFovpDrh7vaIWFDXulfhsdfXf1+IiPuX3VhPHfkw1UlqzwCzgCPrzVdSTae4k2rt4oXA5+rj5gMnAz8CnqUaiX7L6hvt+Dbw1XqKwpeoTqC8gSo4zwB+D/xsBcf/nCpsPlnfvlnXNYVq3vMlVCdWPk510mOpXwH/RfXl4Gmq17wy0zJWx6eplkx8mKr2G6jmREsS4EVSJEmrINbSC6xIUnsceZYkSZIKGZ4lSZKkQk7bkCRJkgo58ixJkiQVMjxLkiRJhUov79oQ73jHO3LAgAGNLkOSJElrualTpz6fme1eaKpLh+cBAwYwZcqURpchSZKktVxEPF2yn9M2JEmSpEKGZ0mSJKmQ4VmSJEkq1KXnPEuSJK2tFi1axKxZs1i4cGGjS1mn9O7dm/79+9OrV69VOt7wLEmS1ACzZs1i0003ZcCAAUREo8tZJ2QmL7zwArNmzWLgwIGr1IfTNiRJkhpg4cKFbLnllgbnThQRbLnllqs12m94liRJahCDc+db3ffc8CxJkiQVMjxLkiR1cQcffDAvvfRSm9sGDBjA888/D8B73/veziyr2L//+7+/5fGarvOll17ie9/73hrp2/AsSZLUxd1222307dv3LW2ZyZtvvvmWtrvvvrszyyq2bHhe03UaniVJktYRhx56KMOHD2fIkCH84Ac/AP4+utzc3MygQYM4+eST2XPPPZk5c+Zbjt1kk00A+N3vfsfo0aM57LDD2HXXXTnmmGPITACmTp3Kfvvtx/DhwznggAOYM2fOcmu56KKLGDx4MMOGDeOoo44C4NVXX+X4449n5MiR7LHHHtxyyy0AXHXVVXzsYx/jwAMPZOedd+bLX/4yAOPHj+f111+nqamJY4455m117rfffhxxxBG8+93vZvz48UyYMIFRo0YxdOhQnnjiCQDmzZvHxz/+cUaOHMnIkSP54x//CMDZZ5/N8ccfz+jRo9lxxx256KKLlj7nE088QVNTE6effvpqfiLLyMwuexs+fHhKkiStjR5++OE221944YXMzHzttddyyJAh+fzzz+cOO+yQ8+bNy6eeeiojIidPnrx0/5ZtmZkbb7xxZmZOmjQp+/TpkzNnzswlS5bk3nvvnXfddVe+8cYb+Z73vCefe+65zMycOHFiHnfcccutceutt86FCxdmZuZf//rXzMz8yle+kj/96U+Xtu288875yiuv5I9//OMcOHBgvvTSS/n666/n9ttvn88888xb6mrRus7NNtssZ8+enQsXLsxtttkmv/a1r2Vm5gUXXJCnnXZaZmYeffTRedddd2Vm5tNPP5277rprZmaeddZZ+Z73vCcXLlyY8+bNyy222CLfeOONfOqpp3LIkCEr9d4DU7Ign7rOsyRJUhdy0UUXcfPNNwMwc+ZMHnvssbds32GHHdh7773b7WfUqFH0798fgKamJpqbm+nbty9/+ctf+OAHPwjAkiVL2HrrrZfbx7BhwzjmmGM49NBDOfTQQwG4/fbbufXWWznvvPOAasm9Z555BoD999+fzTbbDIDBgwfz9NNPs912262wzpEjRy6tYaeddmLMmDEADB06lEmTJgHwm9/8hocffnjpMS+//DILFiwA4JBDDmGDDTZggw02YKuttmLu3Lntvjerw/AsSZLURfzud7/jN7/5DZMnT2ajjTZi9OjRb1uTeOONNy7qa4MNNlh6v0ePHixevJjMZMiQIUyePLmoj1/+8pfceeed3HrrrXzjG99g+vTpZCY33ngju+yyy1v2vffee9t8zpWpc7311lv6eL311lt6/JtvvsnkyZPZcMMNi17nmuScZ0mSpC5i/vz5bL755my00UY88sgj3HPPPR3a/y677MK8efOWhudFixYxffr0Nvd98803mTlzJu973/s499xzeemll3jllVc44IADuPjii5fOof7zn//c7vP26tWLRYsWrXLdY8aM4ZJLLln6eNq0aSvcf9NNN106Mt3RDM+SJEldxIEHHsjixYsZNmwYZ555ZtH0jJWx/vrrc8MNN/Cv//qv7L777jQ1NS135YslS5bwyU9+kqFDh7LHHnvwhS98gb59+3LmmWeyaNEihg0bxm677caZZ57Z7vOOGzdu6RSQVXHRRRcxZcoUhg0bxuDBg7n88stXuP+WW27JPvvsw2677dbhJwxGy7eGrmjEiBE5ZcqURpchSZLU4WbMmMGgQYMaXcY6qa33PiKmZuaI9o515FmSJEkq5AmDkiRJ67hTTjll6drJLU477TSOO+64BlXUdRmeJUlahw0Y/8sO6af5nEM6pB81xqWXXtroEroNp21IkiRJhQzPkiRJUiHDsyRJklTI8CxJkqSGeOmll/je97639PHs2bM57LDDGlhR+zxhUJIkqRvoqJM7W3SFkzxbwvPJJ58MwDbbbMMNN9zQ4KpWzJFnSZIktam5uZlBgwZxwgknMGTIEMaMGcPrr7/OE088wYEHHsjw4cPZd999eeSRRwB44okn2HvvvRk5ciRf+9rX2GSTTQB45ZVX2H///dlzzz0ZOnQot9xyCwDjx4/niSeeoKmpidNPP53m5mZ22203APbaa6+3XDp89OjRTJ06lVdffZXjjz+ekSNHssceeyztq7MYniVJkrRcjz32GKeccgrTp0+nb9++3HjjjYwbN46LL76YqVOnct555y0dOT7ttNM47bTTuO+++9hmm22W9tG7d29uvvlm7r//fiZNmsQXv/hFMpNzzjmHnXbaiWnTpvEf//Efb3neo446iuuuuw6AOXPmMHv2bIYPH863vvUt3v/+93PfffcxadIkTj/9dF599dVOez8Mz5IkSVqugQMH0tTUBMDw4cNpbm7m7rvv5vDDD6epqYkTTzyROXPmADB58mQOP/xwAD7xiU8s7SMzOeOMMxg2bBgf+MAHePbZZ5k7d+4Kn/eII47g+uuvB+C6665b2u/tt9/OOeecQ1NTE6NHj2bhwoU888wzHf66l8c5z5IkSVquDTbYYOn9Hj16MHfuXPr27cu0adOK+5gwYQLz5s1j6tSp9OrViwEDBrBw4cIVHrPtttuy5ZZb8uCDD3Lttdfy/e9/H6iC+I033sguu+yyai9oNTnyLEmSpGJ9+vRh4MCBS0eFM5MHHngAgL333psbb7wRgIkTJy49Zv78+Wy11Vb06tWLSZMm8fTTTwOw6aabsmDBguU+11FHHcW5557L/PnzGTp0KAAHHHAAF198MZkJwJ///OeOf5ErYHiWJEnSSpkwYQJXXHEFu+++O0OGDFl60t4FF1zAd7/7XUaNGsWcOXPYbLPNADjmmGOYMmUKI0aMYMKECey6664AbLnlluyzzz7stttunH766W97nsMOO4yJEydyxBFHLG0788wzWbRoEcOGDWO33XbjzDPP7IRX/HfRktq7ohEjRuSUKVMaXYYkSWutjlr+rCsse9bdzJgxg0GDBjW6jA712muvseGGGxIRTJw4kWuuuabTV8Mo0dZ7HxFTM3NEe8c651mSJEkdYurUqZx66qlkJn379uXKK69sdEkdzvAsSZKkDrHvvvsunf+8tnLOsyRJklTI8CxJkiQVajc8R8R2ETEpImZExPSIOK1uPzsino2IafXt4FbHfCUiHo+IRyPigFbtB9Ztj0fE+DXzkiRJkqQ1o2TO82Lgi5l5f0RsCkyNiF/X287PzPNa7xwRg4GjgCHANsBvIuLd9eZLgQ8Cs4D7IuLWzHy4I16IJEmStKa1O/KcmXMy8/76/gJgBrDtCg75CDAxM/+WmU8BjwOj6tvjmflkZr4BTKz3lSRJ0lqsubmZn//856t07CabbNLB1ayelVptIyIGAHsA9wL7AKdGxKeBKVSj03+lCtb3tDpsFn8P2zOXad9rlaqWJEla15y9WQf3N79j+1uBlvD8iU984m3bFi9eTM+e3WcBuOITBiNiE+BG4POZ+TJwGbAT0ATMAb7Tsmsbh+cK2pd9nnERMSUipsybN6+0PEmSJHWw5uZmBg0axAknnMCQIUMYM2YMr7/+Ok888QQHHnggw4cPZ9999+WRRx4B4Nhjj+WGG25YenzLqPH48eO56667aGpq4vzzz+eqq67i8MMP58Mf/jBjxozhlVdeYf/992fPPfdk6NChXfLCKi2KwnNE9KIKzhMy8yaAzJybmUsy803gh1TTMqAaUd6u1eH9gdkraH+LzPxBZo7IzBH9+vVb2dcjSZKkDvTYY49xyimnMH36dPr27cuNN97IuHHjuPjii5k6dSrnnXceJ5988gr7OOecc9h3332ZNm0aX/jCFwCYPHkyV199Nb/97W/p3bs3N998M/fffz+TJk3ii1/8Il31KtjtjpFHRABXADMy87ut2rfOzDn1w48Cf6nv3wr8PCK+S3XC4M7An6hGnneOiIHAs1QnFb597F6SJEldxsCBA2lqagJg+PDhNDc3c/fdd3P44Ycv3edvf/vbSvf7wQ9+kC222AKAzOSMM87gzjvvZL311uPZZ59l7ty5vOtd7+qYF9GBSiaY7AN8CngoIqbVbWcAR0dEE9XUi2bgRIDMnB4R1wEPU63UcUpmLgGIiFOBXwE9gCszc3oHvhZJkiR1sA022GDp/R49ejB37lz69u3LtGnT3rZvz549efPNN4EqEL/xxhvL7XfjjTdeen/ChAnMmzePqVOn0qtXLwYMGMDChQs78FV0nJLVNv6QmZGZwzKzqb7dlpmfysyhdfs/txqFJjO/lZk7ZeYumflfrdpvy8x319u+taZelCRJktaMPn36MHDgQK6//nqgCsktl+QeMGAAU6dOBeCWW25h0aJFAGy66aYsWLBguX3Onz+frbbail69ejFp0iSefvrpNfwqVp1XGJQkSdJKmTBhAldccQW77747Q4YMWXqC3wknnMDvf/97Ro0axb333rt0dHnYsGH07NmT3XffnfPPP/9t/R1zzDFMmTKFESNGMGHCBHbddddOfT0rI7rqZGyAESNG5JQpUxpdhiRJa60B43/ZIf00n3NIh/SzLpkxYwaDBg1qdBnrpLbe+4iYmpkj2jvWkWdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJ0hpz+eWX85Of/ASAq666itmzZy/d9tnPfpaHH364UaWtkpLLc0uSJKnBhl49tEP7e2jsQx3a3/KcdNJJS+9fddVV7LbbbmyzzTYA/OhHP+qUGjqSI8+SJElqU3NzM7vuuitjx45l2LBhHHbYYbz22mvccccd7LHHHgwdOpTjjz+ev/3tbwCMHz+ewYMHM2zYML70pS8BcPbZZ3Peeedxww03MGXKFI455hiampp4/fXXGT16NFOmTOGyyy7jy1/+8tLnveqqq/jc5z4HwM9+9jNGjRpFU1MTJ554IkuWLOn8N6IVw7MkSZKW69FHH2XcuHE8+OCD9OnTh+9+97sce+yxXHvttTz00EMsXryYyy67jBdffJGbb76Z6dOn8+CDD/LVr371Lf0cdthhSy+/PW3aNDbccMO3bLvpppuWPr722ms58sgjmTFjBtdeey1//OMfmTZtGj169GDChAmd9trbYniWJEnScm233Xbss88+AHzyk5/kjjvuYODAgbz73e8GYOzYsdx555306dOH3r1789nPfpabbrqJjTbaqPg5+vXrx4477sg999zDCy+8wKOPPso+++zDHXfcwdSpUxk5ciRNTU3ccccdPPnkk2vkdZZyzrMkSZKWKyKK9uvZsyd/+tOfuOOOO5g4cSKXXHIJv/3tb4uf58gjj+S6665j11135aMf/SgRQWYyduxYvv3tb69q+R3OkWdJkiQt1zPPPMPkyZMBuOaaa/jABz5Ac3Mzjz/+OAA//elP2W+//XjllVeYP38+Bx98MBdccAHTpk17W1+bbropCxYsaPN5Pvaxj/GLX/yCa665hiOPPBKA/fffnxtuuIHnnnsOgBdffJGnn356TbzMYo48S5IkabkGDRrE1VdfzYknnsjOO+/MhRdeyN57783hhx/O4sWLGTlyJCeddBIvvvgiH/nIR1i4cCGZyfnnn/+2vo499lhOOukkNtxww6WBvMXmm2/O4MGDefjhhxk1ahQAgwcP5pvf/CZjxozhzTffpFevXlx66aXssMMOnfLa2xKZ2bAnb8+IESNyypQpjS5DkqS11oDxv+yQfprPOaRD+lmXzJgxg0GDBjW6jBVqbm7mQx/6EH/5y18aXUqHauu9j4ipmTmivWOdtiFJkiQVMjxLkiSpTQMGDFjrRp1Xl+FZkiRJKmR4liRJapCufO7Z2mp133PDsyRJUgP07t2bF154wQDdiTKTF154gd69e69yHy5VJ0mS1AD9+/dn1qxZzJs3r9GlrFN69+5N//79V/l4w7MkSVID9OrVi4EDBza6DK0kp21IkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQV8iIpkqTVMmD8Lzusr+ZzDumwviRpTXDkWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqVC74TkitouISRExIyKmR8RpdfsWEfHriHis/rt53R4RcVFEPB4RD0bEnq36Glvv/1hEjF1zL0uSJEnqeCUjz4uBL2bmIGBv4JSIGAyMB+7IzJ2BO+rHAAcBO9e3ccBlUIVt4CxgL2AUcFZL4JYkSZK6g3bDc2bOycz76/sLgBnAtsBHgKvr3a4GDq3vfwT4SVbuAfpGxNbAAcCvM/PFzPwr8GvgwA59NZIkSdIatFJzniNiALAHcC/wzsycA1XABraqd9sWmNnqsFl12/LaJUmSpG6hODxHxCbAjcDnM/PlFe3aRluuoH3Z5xkXEVMiYsq8efNKy5MkSZLWuKLwHBG9qILzhMy8qW6eW0/HoP77XN0+C9iu1eH9gdkraH+LzPxBZo7IzBH9+vVbmdciSZIkrVElq20EcAUwIzO/22rTrUDLihljgVtatX+6XnVjb2B+Pa3jV8CYiNi8PlFwTN0mSZIkdQs9C/bZB/gU8FBETKvbzgDOAa6LiM8AzwCH19tuAw4GHgdeA44DyMwXI+IbwH31fl/PzBc75FVIkiRJnaDd8JyZf6Dt+coA+7exfwKnLKevK4ErV6ZASZIkqavwCoOSJElSIcOzJEmSVKhkzvM6Y8D4X3ZYX83nHNJhfUmSJKlrcORZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRC7YbniLgyIp6LiL+0ajs7Ip6NiGn17eBW274SEY9HxKMRcUCr9gPrtscjYnzHvxRJkiRpzSoZeb4KOLCN9vMzs6m+3QYQEYOBo4Ah9THfi4geEdEDuBQ4CBgMHF3vK0mSJHUbPdvbITPvjIgBhf19BJiYmX8DnoqIx4FR9bbHM/NJgIiYWO/78EpXLEmSJDXI6sx5PjUiHqyndWxet20LzGy1z6y6bXntkiRJUrexquH5MmAnoAmYA3ynbo829s0VtL9NRIyLiCkRMWXevHmrWJ4kSZLU8VYpPGfm3MxckplvAj/k71MzZgHbtdq1PzB7Be1t9f2DzByRmSP69eu3KuVJkiRJa8QqheeI2LrVw48CLStx3AocFREbRMRAYGfgT8B9wM4RMTAi1qc6qfDWVS9bkiRJ6nztnjAYEdcAo4F3RMQs4CxgdEQ0UU29aAZOBMjM6RFxHdWJgIuBUzJzSd3PqcCvgB7AlZk5vcNfjSRJkrQGlay2cXQbzVesYP9vAd9qo/024LaVqk6SJEmg43wUAAAeBUlEQVTqQrzCoCRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUiHDsyRJklTI8CxJkiQVMjxLkiRJhQzPkiRJUqGejS5AkgAGjP9lh/TTfM4hHdKPJEltceRZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKtRueI6IKyPiuYj4S6u2LSLi1xHxWP1387o9IuKiiHg8Ih6MiD1bHTO23v+xiBi7Zl6OJEmStOaUjDxfBRy4TNt44I7M3Bm4o34McBCwc30bB1wGVdgGzgL2AkYBZ7UEbkmSJKm7aDc8Z+adwIvLNH8EuLq+fzVwaKv2n2TlHqBvRGwNHAD8OjNfzMy/Ar/m7YFckiRJ6tJWdc7zOzNzDkD9d6u6fVtgZqv9ZtVty2uXJEmSuo2OPmEw2mjLFbS/vYOIcRExJSKmzJs3r0OLkyRJklbHqobnufV0DOq/z9Xts4DtWu3XH5i9gva3ycwfZOaIzBzRr1+/VSxPkiRJ6nirGp5vBVpWzBgL3NKq/dP1qht7A/PraR2/AsZExOb1iYJj6jZJkiSp2+jZ3g4RcQ0wGnhHRMyiWjXjHOC6iPgM8AxweL37bcDBwOPAa8BxAJn5YkR8A7iv3u/rmbnsSYiSJElSl9ZueM7Mo5ezaf829k3glOX0cyVw5UpVJ0mSJHUhXmFQkiRJKmR4liRJkgq1O21D6i4GjP9lh/XVfM4hHdaXJElaezjyLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSoZ6NLkDqks7erIP6md8x/UiSpC7BkWdJkiSpkOFZkiRJKmR4liRJkgo551mSJK2+jjpXBDxfRF2aI8+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFPGFQ0trFk5YkSWuQI8+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVKhnowuQpK5q6NVDO6Sfh8Y+1CH9SJIaz5FnSZIkqZDhWZIkSSpkeJYkSZIKOedZWoM6as4sOG9WkqSuwJFnSZIkqZDhWZIkSSq0WuE5Ipoj4qGImBYRU+q2LSLi1xHxWP1387o9IuKiiHg8Ih6MiD074gVIkiRJnaUjRp7fl5lNmTmifjweuCMzdwbuqB8DHATsXN/GAZd1wHNLkiRJnWZNTNv4CHB1ff9q4NBW7T/Jyj1A34jYeg08vyRJkrRGrG54TuD2iJgaEePqtndm5hyA+u9Wdfu2wMxWx86q2yRJkqRuYXWXqtsnM2dHxFbAryPikRXsG2205dt2qkL4OIDtt99+NcuTJEmSOs5qjTxn5uz673PAzcAoYG7LdIz673P17rOA7Vod3h+Y3UafP8jMEZk5ol+/fqtTniRJktShVnnkOSI2BtbLzAX1/THA14FbgbHAOfXfW+pDbgVOjYiJwF7A/JbpHZIkSS28wJS6stWZtvFO4OaIaOnn55n53xFxH3BdRHwGeAY4vN7/NuBg4HHgNeC41XhuSZIkqdOtcnjOzCeB3dtofwHYv432BE5Z1eeTJEmSGs0rDEqSJEmFVne1DS3P2Zt1YF/zO64vSZIkrTJHniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgoZniVJkqRChmdJkiSpkOFZkiRJKmR4liRJkgr1bHQBkiQtdfZmHdTP/I7pR5KW4cizJEmSVMjwLEmSJBUyPEuSJEmFDM+SJElSIcOzJEmSVMjwLEmSJBVyqbpuYOjVQzukn4fGPtQh/UiSJK2rHHmWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEJenluStNYZevXQDuvrobEPdVhfkro/R54lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIKGZ4lSZKkQoZnSZIkqZDhWZIkSSpkeJYkSZIK9Wx0AZIkSVp5A8b/ssP6aj7nkA7ra23nyLMkSZJUyJFnSZKkdd3Zm3VgX/M7rq8uyJFnSZIkqZDhWZIkSSrktA1JkiR1mKFXD+2Qfh4a+1CH9NPRHHmWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKmQ4VmSJEkqZHiWJEmSChmeJUmSpEKGZ0mSJKlQp4fniDgwIh6NiMcjYnxnP78kSZK0qjo1PEdED+BS4CBgMHB0RAzuzBokSZKkVdXZI8+jgMcz88nMfAOYCHykk2uQJEmSVklnh+dtgZmtHs+q2yRJkqQuLzKz854s4nDggMz8bP34U8CozPxcq33GAePqh7sAj3ZagZ3vHcDzjS5Cq8zPr/vys+ve/Py6Nz+/7mtt/+x2yMx+7e3UszMqaWUWsF2rx/2B2a13yMwfAD/ozKIaJSKmZOaIRtehVePn13352XVvfn7dm59f9+VnV+nsaRv3ATtHxMCIWB84Cri1k2uQJEmSVkmnjjxn5uKIOBX4FdADuDIzp3dmDZIkSdKq6uxpG2TmbcBtnf28XdQ6MT1lLebn13352XVvfn7dm59f9+VnRyefMChJkiR1Z16eW5IkSSpkeJYkSZIKGZ4lrRMiYoOSNnU9EbFTy2cVEaMj4l8iom+j65K0bjI8S1pXTC5sU9dzI7AkIv4BuAIYCPy8sSWpRERssaJbo+vTikXE3hFxX0S8EhFvRMSSiHi50XU1WqevtrEui4h3A6cDO9Dqvc/M9zesKK2UiNgZ+DYwGOjd0p6ZOzasKK1QRLwL2BbYMCL2AKLe1AfYqGGFaWW8WS91+lHggsy8OCL+3OiiVGQqkFT/u9se+Gt9vy/wDNUXIXVdl1Bdk+N6YATwaeAfGlpRF2B47lzXA5cDPwSWNLgWrZofA2cB5wPvA47j72FMXdMBwLFUVzT9Dn//vBYAZzSoJq2cRRFxNDAW+HDd1quB9ahQZg4EiIjLgVvr5WqJiIOADzSyNpXJzMcjokdmLgF+HBF3N7qmRnOpuk4UEVMzc3ij69Cqa/kMI+KhzBxat92Vmfs2ujatWER8PDNvbHQdWnkRMRg4CZicmddExEDgyMw8p8GlqVBb//55qeeuLyLupPqS8yPgf4A5wLGZuXtDC2sw5zx3rv+MiJMjYmvnfHVbCyNiPeCxiDi1/hl5q0YXpSL9I6JPVH4UEfdHxJhGF6UiOwKfz8xrADLzKYNzt/N8RHw1IgZExA4R8W/AC40uSu36FFVWPBV4FdgO+HhDK+oCHHnuRBHxVBvN6XzZ7iMiRgIzqObrfYNq3uy5mXlvQwtTuyLigczcPSIOAE4BzgR+nJl7Nrg0tSMifga8h+rEwR9n5owGl6SVVA8UnQX8E9Uc6DuBr2fmiw0tTCsUERsDr2fmm/XjHsAGmflaYytrLMOztBIi4vDMvL69NnU9EfFgZg6LiAuB32XmzRHx58zco9G1qX0R0Qc4muo8g6Q6/+CazFzQ0MK0UiJik8x8pdF1qExE3AN8oOUzi4hNgNsz872NrayxnLbRySJit4g4IiI+3XJrdE1aKV8pbFPXMzUibgcOBn4VEZsCbza4JhXKzJepRp4nAlsDHwXuj4jPNbQwFYmI90bEw8DD9ePdI+J7DS5L7evd+stOfX+dX6XI1TY6UUScBYymWubsNuAg4A/ATxpYlgrUZ4YfDGwbERe12tQHWNyYqrSSPgM0AU9m5msRsSXVKKa6uIj4MHA8sBPwU2BUZj4XERtRTaO6uJH1qcj5VCvf3AqQmQ9ExD81tiQVeDUi9szM+wEiYjjweoNrajjDc+c6DNgd+HNmHhcR76Q6g1Vd32yq9Ur/uf7bYgHwhYZUpJWVVF9cPwR8HdiYVmt1q0s7HDg/M+9s3Vh/CTq+QTVpJWXmzIi3rOzpkq1d3+eB6yNidv14a+DIBtbTJRieO9frmflmRCyu5+89R3UWubq4zHwAeCAifpaZjjR3T9+jmqbxfqrwvIBqGsDIRhal9mXmcqe3ZeYdnVmLVtnMiHgvkBGxPvAvVL8aqAvLzPsiYldgF6o18h/JzEUNLqvhDM+da0pE9KW6SMpU4BXgT40tSSUi4iGqkUuWGTkBIDOHdXZNWml7ZeaeLVemy8y/1v+Iq4uLiL2ppmYMAtYHegCvZmafhhamlXEScCHV1T5nAbcDJze0Ii1XRLw/M38bER9bZtPOEUFm3tSQwroIw3MnysyW/6O4PCL+G+iTmQ82siYV+1CjC9BqW1Qvs9TyJagfnjDYXXiJ4O5vl8w8pnVDROwD/LFB9WjF9gN+y9+v6NlaAut0eHapuk5UX1Djt5k5v37cFxidmb9obGVaGRGxA7BzZv4mIjYEerpcVtcXEcdQzdXbE7ia6hyEr7rMYNfXciW6luUG67a71/XlsrqTiLh/2TXV22pT11FfEOywzLyu0bV0NY48d66zMvPmlgeZ+VK9AofhuZuIiBOAccAWVGf+9wcuB/ZvZF1qX2ZOiIipVJ9VAId6sY1u47V6is20iDiX6hLBGze4JhWIiPcA7wX6RcT/arWpD9X0G3VR9TlapwKG52W4znPnauv99gtM93IKsA/wMkBmPoaX5+4WIuLrVJeWvSozLzE4dyteIrj7Wh/YhOrfuk1b3V6m+vVHXduvI+JLEbFdRGzRcmt0UY3mtI1OFBFXAi8Bl1LNGfocsHlmHtvIulQuIu7NzL1arkwXET2B+z1hsOurlzT7R6rLPC8A7gLuzMxbGlqY2hUR7wfuWdcvCdydRcQOmfl0o+vQyomIp9pozsxcp1cKMzx3ovoa8WcCH6D62fh24JuZ+WpDC1Ox+ifjl6hOWPoc1dniD2fmvzW0MBWLiHcBRwBfovryummDS1I7IuInwN7AC1Rfeu4C/pCZf21oYWpXRFyQmZ+PiP+kPlm3tcz85waUJa0Ww7O0EuoTKD4DjKH6AvQr4Efp/5C6vIj4EdVFUuZShy+qXw1ct7ubiIhtqH7q/xKwTWY67a2Li4jhmTk1IvZra3tm/r6za1K5+iqe/wvYPjPHRcTOVCun/N8Gl9ZQhudO4DfvtUu9xBmZOa/RtahcRNwMbAM8DPyeasrGk42tSiUi4pPAvsBQ4HmqLz53ZebkhhYmreUi4lqq61J8OjN3q1eYmpyZTQ0uraEMz53Ab97dX1RXRjmL6oSlqG9LgIsz8+uNrE0rJyIGAQdQXVa9R2b2b3BJakdEPA88QbWyzaTMbG5sRVpZ9ZrOZwM7UJ08GDh3tstrtUzknzNzj7rtgczcvdG1NZI/eXWCOjj3AE7IzE82uh6tks9TrbIxMjOfAoiIHYHLIuILmXl+Q6tTuyLiQ1Sjl/8EbE51AYC7GlqUimTmOyJiCNVn9636p+NHM/NTDS5N5a6g+sI6lWrgQd3DG/Voc8vFpXYC/tbYkhrP8NxJMnNJRPSLiPUz841G16OV9mngg5n5fEtDZj5Z/5x8O2B47voOAu4ELszM2Y0uRuUiog+wPdWo5QBgM7w6ZHczPzP/q9FFaKWdBfw3sF1ETKAaRDq2oRV1AU7b6EQR8X2qq5vdSrVWKQCZ+d2GFaUiEfGXzNxtZbdJWn0R8SDVPOc/UM1Vn9XgkrSSIuIcqoui3ESrkcvMvL9hRalIRGxJtdpNUC0Z+Xw7h6z1HHnuXLPr23pUi8Sr+1jRrwX+ktANRMTHgP9DdVGblnnrmZl9GlqYVqie8nZ7Zn6p0bVotexV/x1e/w2qqQDvb0w5WgnbUn3x6Qn8U0SQmTc1uKaGcuS5ASJiY9d27l4iYgmtfi1ovQnonZm9OrkkraSIeBz4sFcW7H4i4o7M3L/RdWjltbokd9R/E5hHtU53WxfgUBdSX9xtGDCdv0+Vysw8vnFVNZ4jz50oIt5DddLEJsD2EbE7cGJmntzYytSezOzR6Bq02uYanLutaRFxK3A9b53ytk6PfnUTbf3KugPwbxFxdmZO7OyCtFL2zszBjS6iq3HkuRNFxL1UC/zf2mrJF+fLSmtQPV0DYD/gXcAveOucSwNYFxcRP26jeZ0f/erOImIL4DeZuWeja9HyRcQVwHcy8+FG19KVOPLcyTJzZrVk8FIu2SOtWR9udf81qqtDtkiqE5jUhWXmcY2uQR0rM1+MZf4xVJd0NTA5Iv6HatCh5VyRYY0tq7EMz51rZkS8F8iIWB/4F8CfkaU1qCV4RcQ+mfnH1tvqCzeoi4qIL2fmuRFxMW1fnfVfGlCWOkBEvB/4a6PrULuuBD4FPITLQy5leO5cJwEXUp25OotqfeBTGlqRtO64mGqpyPba1HW0DC5MaWgVWmUR8RBv/+KzBdXKU5/u/Iq0kp7JzFsbXURX45xnSWu1+kTd91JdJbL1xWz6AB9d1y8z2x1FRG+qlVOub3QtWrGI2GGZpgRecMWp7iEivgf0Bf4TzxVZypHnThQRA4HPUV0ha+l7n5n/3KiapHXA+lQr3PTkrWf+v0x1Aq+6gXq95zHA0cABVJdWNzx3cZn5dKNr0GrZkCo0e65IK448d6KIeIBqqbq3zB3KzN83rChpHRERO/gPefcTEf8EfAI4BPgT1eWBd8zM1xpamLSOioiRmXlfo+toJMNzJ4qIezNzr/b3lNTRImISbZ905hXOuqiImAU8A1wG/CIzF0TEU5k5sMGlSeuUiBgMHEX1y8/8zBzR4JIaymkbnevCiDiL6kTB1nOH7m9cSdI6o/XlnXsDHwcWN6gWlbkROBQ4ElgSEbfQxhcgSR2vnq9+dH1bTHVxmxGZ2dzIuroCR547UUR8m2rJlyd462Uu///27j7kzrqO4/j74zayx/XgWkLDYbosxbZUcjV0lmUYEj6kLbDIatAf2gP0T0GIVuAfQWApRUgPqNPSDJrNNDd1iBjZbIgPA3UQSXM+hQ85Xd/+uK779vb23jrWdv/Oznm/4HDOua77PudzM865vvtd3+v3c+RLaiDJLVV1fOsc2rV+LuAT6A7gJ9Nd6PkF4PqqerplNmlUJbkdmA+sAdZU1RbP+rzEkefZdSpdr96O1kGkcdOvaDZhP+AouhUHNcSqG+G5Gbg5yTzg43SF9CXAAS2zSSPsUeCdwEJgAbAFz/pMcuR5FiW5Cji3qra1ziKNmyQP0X35h+4U5EPABVW1sWkw/U+SvLaqnmudQxpVSebTtbetAg6hm7LupKq6s2mwIWDxPIuSbACOBP7ESz3PVVWfbBZKkiRpN5K8ne7ag1XAoqpa1DhSUxbPsyjJ1N7KACuAVVV1eKNI0tjoT/l/GTiu37QB+HFVvdAslCTtY5z20+J51iVZSjdn6Zl0p42vraqL26aSRl+SnwLzgJ/3m84GdlbVF9ul0quV5C3Ak+XBS1IjXjA4C5Is4aX5ER8DrqL7j8sJTYNJ4+WYaUtx39wvXKQhleTbwNVVdV+S1wDrgPcBLyb5TFXd1DahpHG0X+sAY+I+4CPAKVW1oh9p3tk4kzRudiZ518STJAfj53DYnQXc3z/+XH+/ADge+F6TRJLGniPPs+N0upHn9UnW0c2bmLaRpLHzDbrP4IP988XA59vF0QB2TGnPOIluvtmdwL1JPH5Je1mSBcCX6L4vJz9zVXVOq0zDwJHnWVBVv6mqs4DD6C5S+hqwMMmlST7WNJw04pIck+QdVfVH4FDgWuCfdCt92rYx3J5PckR/AD+B7t9swusaZZLGyW/pFku5CVg75TbWvGCwkX7Bhk8BZ7nCoLT3JLkLOLGqHk9yHN2Zn3OBpcB7quqMpgG1S0mOBX5G16rxg6q6sN9+MnB2Va1qGE8aeUk2VdXS1jmGjcWzpJGW5O6JCwWT/Ah4tKrO7597YJCkXUjyHeD2qrq+dZZhYs+YpFE3J8ncqnqR7sLd1VP2+R04xJJ8fdqmArYDG6vqoQaRpHHzFeCbSZ4HXqC7Xquq6k1tY7XlgUPSqLsSuCXJduA54DaAJIcAT7UMpv/qjTNsWwx8K8n5VbVmlvNIY6WqZvoMjj3bNiSNvL539kDgD1X1TL9tCfCGqrqraTi9av01IzdV1ftbZ5FGUZLD+vnVZ/yMjfv3psWzJGmfk+QvVbWsdQ5pFCX5SVWtTrJ+ht017hMd2LYhSdqnJPkw8ETrHNKoqqrV/b0rIc/A4lmSNJSSbKa7SHCqtwJ/Bz47+4mk8ZLktBk2PwVsrqpts51nWNi2IUkaSkkOmrapgMcm+tYl7V1J1gLLgYn2jZXAHcAS4IKq+mWjaE058ixJGkpVtbV1BmnM/ZtuMal/ACRZCFwKfAC4FRjL4tnluSVJkjSTxROFc28bsKSqHqeb93ksOfIsSZKkmdyW5HfAr/rnpwO3Jnk98GS7WG3Z8yxJkqRXSBLgNGAF3eqCG4FrasyLR4tnSZIkvUySOcANVXVi6yzDxp5nSZIkvUxV7QSeTTK/dZZhY8+zJEmSZvIvYHOSG4HJKSKr6rx2kdqzeJYkSdJM1vY3TWHPsyRJkjQgR54lSZI0KcnVVXVmks10K3u+TFUd2SDW0HDkWZIkSZOSHFhVjyQ5aKb94776p8WzJEmSJiX5IXBFVd3eOsswcqo6SZIkTbUF+H6Sh5NclGRp60DDxJFnSZIkvULftvHp/rY/cCWwpqoeaBqsMYtnSZIk7VaSZcBlwJFVNad1npZs25AkSdIrJJmX5JQklwO/Bx4ATm8cqzlHniVJkjQpyUeBVcAngDuBNcB1VfXMbn9xTFg8S5IkaVKS9cAVwDVV9XjrPMPG4lmSJEkakD3PkiRJ0oAsniVJkqQBWTxL0j4kyfVJ3ryLfQ8nOaB/7MpgkrQX2PMsSfu4JAECPAgcXVXbG0eSpJHlyLMkDakk1yX5c5J7kqzutz2c5IAki5Pcm+QS4C5g0bTffbq/X5lkQ5JfJ7kvyeV9sU2So5Lc0r/HDUkOnO2/UZL2NRbPkjS8zqmqo4CjgfOSvG3a/ncDv6iqZVW1dTevswz4KvBe4GDgQ0nmARcDZ/TvcRnw3T3+F0jSiJnbOoAkaZfOS3Jq/3gRcOi0/Vur6o4BXufOqvobQJJNwGLgSeAI4MZ+IHoO8MieCC1Jo8ziWZKGUJKVwInA8qp6NskGYP9pPzboal/PT3m8k+67P8A9VbX8/4wqSWPFtg1JGk7zgSf6wvkw4Ng9/Pr3AwuSLAdIMi/J4Xv4PSRp5Fg8S9JwWgfMTfJX4EJgkPaMgVXVDuAM4KIkdwObgA/uyfeQpFHkVHWSJEnSgBx5liRJkgZk8SxJkiQNyOJZkiRJGpDFsyRJkjQgi2dJkiRpQBbPkiRJ0oAsniVJkqQBWTxLkiRJA/oP6ADqNVAlsKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfac = raw_data[['airline', 'airline_sentiment']]\n",
    "dfac = dfac.groupby(['airline', 'airline_sentiment']).size()\n",
    "dfac = dfac.unstack()\n",
    "dfac.plot.bar(title='Sentiment counts per airline', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution is not the same; some airlines are heavily biased towards negative tweets. Besides, we know that the airlines handlers are present at the beginning of almost all the tweets. This will lead me to remove them in order to avoid the classifiers to learn from a strong, false signal (the airline handler instead of the text sentiment itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing\n",
    "\n",
    "The other \"classic\" techniques that I will use as baseline normally require a more involved pre-processing:\n",
    "- All the tweets have a handler with the airline's company at the beginning (e.g. `@VirginAmerica`), I am removing them as they bring no additional information and actually could make the models to overfit, learning to classify according to the airline's name instead of the actual sentiment of the text.\n",
    "\n",
    "- `nltk` has a very handy `TweetTokenizer` ([documentation](https://www.nltk.org/api/nltk.tokenize.html)) that helps to clean up HTML tags and to respect/normalize emojis.\n",
    "- We remove English stopwords and most punctuation marks, except a few that convey semantic meaning.\n",
    "- Originally this used stemming, but we can also leverage lemmatization from NLTK at a very small cost, and it should in theory offer better quality results.\n",
    "- Any hashtag that looks like a word (e.g. `#angry`) has the initial `#` removed, since they normally carry regular meaning.\n",
    "- All URLs are replaced by `xxurl`.\n",
    "- All numbers ar replaced by `xxnum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "REGEX_URL = re.compile(r'https?://|www.')\n",
    "REGEX_NUM = re.compile(r'[0-9.-]?[0-9][0-9.-]?')\n",
    "\n",
    "TOKEN_URL = 'xxurl'\n",
    "TOKEN_NUM = 'xxnum'\n",
    "\n",
    "STOPWORDS_EN = set(stopwords.words('english') + [\"i've\"])\n",
    "STOP_PUNCT = set('.\"\\'&”“’,:;/*()[]{}@#')\n",
    "\n",
    "def normalize_token(token):\n",
    "    for regex, marker in [(REGEX_URL, TOKEN_URL), (REGEX_NUM, TOKEN_NUM)]:\n",
    "        if re.match(regex, token):\n",
    "            return marker\n",
    "    return token\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def normalize_text(text, remove_stopwords=True, use_lemmatizer=False, remove_prefixes=True):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def is_stop_token(t):\n",
    "        return len(t) == 0 or t in STOP_PUNCT or remove_stopwords and t in STOPWORDS_EN\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # if the first token is a handler, it's normally irrelevant\n",
    "    if tokens[0].startswith('@'):\n",
    "        tokens.pop(0)\n",
    "\n",
    "    ntokens = []\n",
    "    \n",
    "    # STEP 1: cleanup, formatting\n",
    "    for token in tokens:\n",
    "        ntoken = normalize_token(token)\n",
    "        \n",
    "        if remove_prefixes and any(token.startswith(c) for c in ['@', '#']):\n",
    "            ntoken = ntoken[1:]\n",
    "        if not use_lemmatizer and is_stop_token(ntoken):\n",
    "            continue\n",
    "\n",
    "        ntokens.append(ntoken)\n",
    "        \n",
    "    if not use_lemmatizer:\n",
    "        return ' '.join(ntokens)\n",
    "    \n",
    "    ltokens = []\n",
    "    \n",
    "    # STEP 2: NLP tagging\n",
    "    pos_tags = nltk.pos_tag(ntokens)\n",
    "\n",
    "    # STEP 3: \"semantic\" cleaning\n",
    "    for token, pos in pos_tags:\n",
    "        if is_stop_token(token):\n",
    "            continue\n",
    "        if pos == 'CD':\n",
    "            ltoken = TOKEN_NUM\n",
    "        else:\n",
    "            ltoken = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "        ltokens.append(ltoken)\n",
    "           \n",
    "    return ' '.join(ltokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL: @VirginAmerica What @dhepburn said.\n",
      ">> dhepburn said\n",
      "\n",
      "POSITIVE: @VirginAmerica plus you've added commercials to the experience... tacky.\n",
      ">> plus added commercials experience ... tacky\n",
      "\n",
      "NEUTRAL: @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      ">> today ... must mean need take another trip !\n",
      "\n",
      "NEGATIVE: @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      ">> really aggressive blast obnoxious entertainment guests faces little recourse\n",
      "\n",
      "NEGATIVE: @VirginAmerica and it's a really big bad thing about it\n",
      ">> really big bad thing\n",
      "\n",
      "NEGATIVE: @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      ">> seriously would pay $ xxnum flight seats playing really bad thing flying va\n",
      "\n",
      "POSITIVE: @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\n",
      ">> yes nearly every time fly vx ear worm go away :)\n",
      "\n",
      "NEUTRAL: @VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n",
      ">> really missed prime opportunity men without hats parody xxurl\n",
      "\n",
      "POSITIVE: @virginamerica Well, I didn't…but NOW I DO! :-D\n",
      ">> well … ! :-D\n",
      "\n",
      "POSITIVE: @VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\n",
      ">> amazing arrived hour early good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_normalization(_df):\n",
    "    for i, row in _df.iterrows():\n",
    "        print(row.sentiment.upper() + \": \" + row.text + '\\n>> ' + normalize_text(row.text) + '\\n')\n",
    "\n",
    "show_normalization(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we add a new column to the dataset with the normalized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus added commercials experience ... tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>today ... must mean need take another trip !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>seriously would pay $ xxnum flight seats playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes nearly every time fly vx ear worm go away :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>really missed prime opportunity men without ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>positive</td>\n",
       "      <td>well … ! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazing arrived hour early good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica did you know that suicide is th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>know suicide second leading cause death among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>&lt;3 pretty graphics much better minimal iconogr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great deal ! already thinking xxnum trip austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>virginmedia i'm flying fabulous seductive skie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>negative</td>\n",
       "      <td>sfo-pdx schedule still mia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>excited first cross country flight lax mco hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n",
       "      <td>negative</td>\n",
       "      <td>flew nyc sfo last week fully sit seat due two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n",
       "      <td>positive</td>\n",
       "      <td>❤ ️ flying virginamerica ☺ ️ 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>know would amazingly awesome ? bos-fll please ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "0                 @VirginAmerica What @dhepburn said.   neutral   \n",
       "1   @VirginAmerica plus you've added commercials t...  positive   \n",
       "2   @VirginAmerica I didn't today... Must mean I n...   neutral   \n",
       "3   @VirginAmerica it's really aggressive to blast...  negative   \n",
       "4   @VirginAmerica and it's a really big bad thing...  negative   \n",
       "5   @VirginAmerica seriously would pay $30 a fligh...  negative   \n",
       "6   @VirginAmerica yes, nearly every time I fly VX...  positive   \n",
       "7   @VirginAmerica Really missed a prime opportuni...   neutral   \n",
       "8     @virginamerica Well, I didn't…but NOW I DO! :-D  positive   \n",
       "9   @VirginAmerica it was amazing, and arrived an ...  positive   \n",
       "10  @VirginAmerica did you know that suicide is th...   neutral   \n",
       "11  @VirginAmerica I &lt;3 pretty graphics. so muc...  positive   \n",
       "12  @VirginAmerica This is such a great deal! Alre...  positive   \n",
       "13  @VirginAmerica @virginmedia I'm flying your #f...  positive   \n",
       "14                             @VirginAmerica Thanks!  positive   \n",
       "15      @VirginAmerica SFO-PDX schedule is still MIA.  negative   \n",
       "16  @VirginAmerica So excited for my first cross c...  positive   \n",
       "17  @VirginAmerica  I flew from NYC to SFO last we...  negative   \n",
       "18                    I ❤️ flying @VirginAmerica. ☺️👍  positive   \n",
       "19  @VirginAmerica you know what would be amazingl...  positive   \n",
       "\n",
       "                                            norm_text  \n",
       "0                                       dhepburn said  \n",
       "1         plus added commercials experience ... tacky  \n",
       "2        today ... must mean need take another trip !  \n",
       "3   really aggressive blast obnoxious entertainmen...  \n",
       "4                                really big bad thing  \n",
       "5   seriously would pay $ xxnum flight seats playi...  \n",
       "6    yes nearly every time fly vx ear worm go away :)  \n",
       "7   really missed prime opportunity men without ha...  \n",
       "8                                        well … ! :-D  \n",
       "9                     amazing arrived hour early good  \n",
       "10  know suicide second leading cause death among ...  \n",
       "11  <3 pretty graphics much better minimal iconogr...  \n",
       "12  great deal ! already thinking xxnum trip austr...  \n",
       "13  virginmedia i'm flying fabulous seductive skie...  \n",
       "14                                           thanks !  \n",
       "15                         sfo-pdx schedule still mia  \n",
       "16  excited first cross country flight lax mco hea...  \n",
       "17  flew nyc sfo last week fully sit seat due two ...  \n",
       "18                     ❤ ️ flying virginamerica ☺ ️ 👍  \n",
       "19  know would amazingly awesome ? bos-fll please ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply normalization to ALL text\n",
    "df['norm_text'] = df.text.apply(normalize_text)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually check the distribution of tokens grouped by the sentiment of the tweet. Since we have already normalized them and done a lot of cleanup (like removing stopwords, etc.) this should be quite easy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENTS = ['positive', 'neutral', 'negative']\n",
    "token_counters = {sent: Counter() for sent in SENTIMENTS}\n",
    "for _, row in df.iterrows():\n",
    "    tokens = row['norm_text'].split(' ')\n",
    "    token_counters[row['sentiment']].update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE            \t\tNEUTRAL             \t\tNEGATIVE            \n",
      "!               1947\t\t?               1385\t\txxnum           5102\n",
      "thanks           610\t\txxnum           1075\t\t?               3106\n",
      "thank            453\t\t!                647\t\tflight          2925\n",
      "xxnum            414\t\tflight           609\t\t!               2486\n",
      "flight           377\t\txxurl            526\t\tget              988\n",
      "great            236\t\t-                244\t\tcancelled        925\n",
      "xxurl            233\t\tget              238\t\t...              747\n",
      "service          162\t\tplease           181\t\tservice          744\n",
      "love             136\t\t...              179\t\thours            649\n",
      "-                125\t\tflights          169\t\thelp             617\n",
      "?                117\t\thelp             167\t\thold             611\n",
      "customer         114\t\tneed             164\t\tcustomer         608\n",
      "get              114\t\tthanks           156\t\ttime             588\n",
      "guys             110\t\tjetblue          146\t\t-                584\n",
      "much             109\t\twould            122\t\tplane            521\n",
      ":)               108\t\tdm               122\t\tdelayed          505\n",
      "good             108\t\ti'm              119\t\ti'm              499\n",
      "...              105\t\tus               107\t\tstill            489\n",
      "best             105\t\tfleek            107\t\tus               480\n",
      "awesome          100\t\ttomorrow         106\t\tcall             458\n",
      "got              100\t\tknow             104\t\txxurl            452\n",
      "time              95\t\tfleet's          102\t\tcan't            448\n",
      "jetblue           94\t\tcancelled        101\t\tflightled        446\n",
      "us                87\t\ttime              92\t\thour             445\n",
      "help              83\t\tway               92\t\tone              436\n"
     ]
    }
   ],
   "source": [
    "mct = [[(s.upper(), '')] + token_counters[s].most_common(25) for s in SENTIMENTS]\n",
    "for i in range(len(mct[0])):\n",
    "    row = ['%-15s%5s' % c[i] for c in mct]\n",
    "    print('\\t\\t'.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be important to remember these when we study the impact of individual words in the sentiment of the sentence. This observation also hints that a classifier that relies on just word distributions (li "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset with only pos/neg: (11541, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus added commercials experience ... tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>seriously would pay $ xxnum flight seats playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes nearly every time fly vx ear worm go away :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "1  @VirginAmerica plus you've added commercials t...  positive   \n",
       "3  @VirginAmerica it's really aggressive to blast...  negative   \n",
       "4  @VirginAmerica and it's a really big bad thing...  negative   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...  negative   \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...  positive   \n",
       "\n",
       "                                           norm_text  \n",
       "1        plus added commercials experience ... tacky  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  \n",
       "5  seriously would pay $ xxnum flight seats playi...  \n",
       "6   yes nearly every time fly vx ear worm go away :)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary = df[df.sentiment != 'neutral']\n",
    "print(\"Size of the dataset with only pos/neg: \" + str(df_binary.shape))\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training of all the examples, we will split 10% of the data for the validation (test) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 10386; test size: 1155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(df_binary['norm_text'], df_binary['sentiment'], test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"Training size: %d; test size: %d\" % (len(text_train), len(text_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preparation for smaller representation of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "\n",
    "y_train = [LABELS[l] for l in label_train]\n",
    "y_test = [LABELS[l] for l in label_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the performance of all the classifiers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key : (train score, test score)\n",
    "results = {}\n",
    "\n",
    "def add_result(*args):\n",
    "    results[args[0]] = args[1:]\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier\n",
    "\n",
    "As an absolute baseline, let's check the accuracy of a classifier that just outputs random predictions based on the training set's class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('random_with_class_dist', 0.6746581937223185, 0.6588744588744588)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='stratified', random_state=42)\n",
    "clf.fit(text_train, y_train)\n",
    "\n",
    "score_train = clf.score(text_train, y_train)\n",
    "score_test = clf.score(text_test, y_test)\n",
    "\n",
    "add_result('random_with_class_dist', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomian Naive Bayes using counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first approach, we use directly the word counts as features, and feed them into a multinomial Naive Bayes classifier from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "X_train = counter.fit_transform(text_train)\n",
    "X_test = counter.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with word counts as features (num_samples, num_features): (10386, 10201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the training set with word counts as features (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_multi_counts', 0.9433853264009243, 0.9038961038961039)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_multi_counts', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB using TF-IDF, direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with TF-IDF vectorization (num_samples, num_features): (10386, 9981)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "v_train = vectorizer.fit_transform(text_train)\n",
    "v_test = vectorizer.transform(text_test)\n",
    "\n",
    "X_train = v_train.toarray()\n",
    "X_test = v_test.toarray()\n",
    "\n",
    "print(\"Size of the training set with TF-IDF vectorization (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_gaussian_tf_idf', 0.8290968611592529, 0.6874458874458874)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_gaussian_tf_idf', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB using TF-IDF and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with TF-IDF vectorization and feature selection (num_samples, num_features): (10386, 998)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(v_train, y_train)\n",
    "vs_train = selector.transform(v_train)\n",
    "vs_test  = selector.transform(v_test)\n",
    "\n",
    "X_train = vs_train.toarray()\n",
    "X_test = vs_test.toarray()\n",
    "\n",
    "print(\"Size of the training set with TF-IDF vectorization and feature selection (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_gaussian_tf_idf_feature_sel', 0.865106874638937, 0.8112554112554112)\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_gaussian_tf_idf_feature_sel', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC would use these best parameters: {'C': 10.0, 'kernel': 'linear'}, train score=0.951906, test score=0.914982\n",
      "('SVC_linear_10.0', 0.951906488272142, 0.9149817061428847)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we try different values of C and kernel\n",
    "parameters = {\n",
    "    'kernel': ('linear', 'rbf'),\n",
    "    'C': [1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "svc = SVC(gamma='auto') # setting gamma=auto to avoid warning\n",
    "clf = GridSearchCV(svc, parameters, cv=3, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the best and put it in the results\n",
    "score_train = clf.cv_results_['mean_train_score'][clf.best_index_]\n",
    "score_test = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "print(\"SVC would use these best parameters: %s, train score=%f, test score=%f\" % (clf.best_params_, score_train, score_test))\n",
    "add_result('SVC_%s_%.1f' % (clf.best_params_['kernel'], clf.best_params_['C']), score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.903235</td>\n",
       "      <td>25.123315</td>\n",
       "      <td>9.672832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.794724</td>\n",
       "      <td>0.794724</td>\n",
       "      <td>33.335429</td>\n",
       "      <td>13.849491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951906</td>\n",
       "      <td>0.914982</td>\n",
       "      <td>20.304209</td>\n",
       "      <td>7.031278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10</td>\n",
       "      <td>0.813980</td>\n",
       "      <td>0.813595</td>\n",
       "      <td>34.113281</td>\n",
       "      <td>13.956996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.957395</td>\n",
       "      <td>0.912767</td>\n",
       "      <td>22.295236</td>\n",
       "      <td>5.947098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>100</td>\n",
       "      <td>0.877431</td>\n",
       "      <td>0.871750</td>\n",
       "      <td>29.860306</td>\n",
       "      <td>12.008685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_kernel param_C  mean_train_score  mean_test_score  mean_fit_time  \\\n",
       "0       linear       1          0.921240         0.903235      25.123315   \n",
       "1          rbf       1          0.794724         0.794724      33.335429   \n",
       "2       linear      10          0.951906         0.914982      20.304209   \n",
       "3          rbf      10          0.813980         0.813595      34.113281   \n",
       "4       linear     100          0.957395         0.912767      22.295236   \n",
       "5          rbf     100          0.877431         0.871750      29.860306   \n",
       "\n",
       "   mean_score_time  \n",
       "0         9.672832  \n",
       "1        13.849491  \n",
       "2         7.031278  \n",
       "3        13.956996  \n",
       "4         5.947098  \n",
       "5        12.008685  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_df = pd.DataFrame(clf.cv_results_)[['param_kernel', 'param_C', 'mean_train_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time']]\n",
    "svc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dtree', 0.952050837666089, 0.8805194805194805)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=40)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('dtree', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>score_train</th>\n",
       "      <th>score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_with_class_dist</td>\n",
       "      <td>0.674658</td>\n",
       "      <td>0.658874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_multi_counts</td>\n",
       "      <td>0.943385</td>\n",
       "      <td>0.903896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nb_gaussian_tf_idf</td>\n",
       "      <td>0.829097</td>\n",
       "      <td>0.687446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nb_gaussian_tf_idf_feature_sel</td>\n",
       "      <td>0.865107</td>\n",
       "      <td>0.811255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC_linear_10.0</td>\n",
       "      <td>0.951906</td>\n",
       "      <td>0.914982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dtree</td>\n",
       "      <td>0.952051</td>\n",
       "      <td>0.880519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           method  score_train  score_test\n",
       "0          random_with_class_dist     0.674658    0.658874\n",
       "1                 nb_multi_counts     0.943385    0.903896\n",
       "2              nb_gaussian_tf_idf     0.829097    0.687446\n",
       "3  nb_gaussian_tf_idf_feature_sel     0.865107    0.811255\n",
       "4                 SVC_linear_10.0     0.951906    0.914982\n",
       "5                           dtree     0.952051    0.880519"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame([(k, *v) for k, v in results.items()], columns=['method', 'score_train', 'score_test'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
