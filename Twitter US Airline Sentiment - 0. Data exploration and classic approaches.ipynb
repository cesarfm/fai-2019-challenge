{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Part 0: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I just explore the data to understand better its properties and distribution, experiment with some ideas for text pre-processing, and use a number of \"classic\" ML methods to obtain a baseline for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SIZE: (14640, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0                @VirginAmerica What @dhepburn said.   neutral\n",
       "1  @VirginAmerica plus you've added commercials t...  positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...   neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...  negative\n",
       "4  @VirginAmerica and it's a really big bad thing...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from the CSV file\n",
    "INPUT_PATH = 'twitter-airline-sentiment/Tweets.csv'\n",
    "raw_data = pd.read_csv(INPUT_PATH, header=0)\n",
    "\n",
    "df = raw_data.copy()[['text', 'airline_sentiment']]\n",
    "df = df.rename(columns={'airline_sentiment': 'sentiment'})\n",
    "\n",
    "print(\"DATA SIZE: \" + str(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "negative   9178\n",
       "neutral    3099\n",
       "positive   2363"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset has 14640 tweets, of which the big majority (9178) are negative, 2363 are positive and there are some neutrals. We also see that are many Twitter handlers, which normally refer to entities. Let's check them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of handlers = 889, with more than 1 occurrence = 193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@united', 3893),\n",
       " ('@usairways', 2998),\n",
       " ('@americanair', 2961),\n",
       " ('@southwestair', 2458),\n",
       " ('@jetblue', 2248),\n",
       " ('@virginamerica', 518),\n",
       " ('@delta', 68),\n",
       " ('@imaginedragons', 45),\n",
       " ('@phlairport', 20),\n",
       " ('@dfwairport', 17),\n",
       " ('@wsj', 13),\n",
       " ('@ladygaga', 12),\n",
       " ('@carrieunderwood', 12),\n",
       " ('@fortunemagazine', 12),\n",
       " ('@love_dragonss', 10),\n",
       " ('@virginatlantic', 9),\n",
       " ('@flytpa', 9),\n",
       " ('@cowboycerrone', 9),\n",
       " ('@staralliance', 8),\n",
       " ('@gg8929', 8)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find the handlers\n",
    "from collections import Counter\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "def get_text_sequence(_df):\n",
    "    return (row.text for _, row in _df.iterrows())\n",
    "\n",
    "def get_all_handlers(text_it):\n",
    "    handler_counts = Counter()\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    \n",
    "    for text in text_it:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        handler_counts.update(t for t in tokens if len(t) > 1 and t.startswith('@'))\n",
    "\n",
    "    return handler_counts\n",
    "\n",
    "handler_counts = get_all_handlers(get_text_sequence(df))\n",
    "num_handlers_more_1 = sum(1 if c > 1 else 0 for c in handler_counts.values())\n",
    "print(\"Number of handlers = %d, with more than 1 occurrence = %d\" % (len(handler_counts), num_handlers_more_1))\n",
    "handler_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge majority of handlers refer to the airlines themselves, also considering that every tweet starts with one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing\n",
    "\n",
    "The other \"classic\" techniques that I will use as baseline normally require a more involved pre-processing:\n",
    "- All the tweets have a handler with the airline's company at the beginning (e.g. `@VirginAmerica`), I am removing them as they bring no additional information and actually could make the models to overfit, learning to classify according to the airline's name instead of the actual sentiment of the text.\n",
    "\n",
    "- `nltk` has a very handy `TweetTokenizer` ([documentation](https://www.nltk.org/api/nltk.tokenize.html)) that helps to clean up HTML tags and to respect/normalize emojis.\n",
    "- We remove English stopwords and most punctuation marks, except a few that convey semantic meaning.\n",
    "- Originally this used stemming, but we can also leverage lemmatization from NLTK at a very small cost, and it should in theory offer better quality results.\n",
    "- Any hashtag that looks like a word (e.g. `#angry`) has the initial `#` removed, since they normally carry regular meaning.\n",
    "- All URLs are replaced by `xxurl`.\n",
    "- All numbers ar replaced by `xxnum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "REGEX_URL = re.compile(r'https?://|www.')\n",
    "REGEX_NUM = re.compile(r'[0-9.-]?[0-9][0-9.-]?')\n",
    "\n",
    "TOKEN_URL = 'xxurl'\n",
    "TOKEN_NUM = 'xxnum'\n",
    "\n",
    "STOPWORDS_EN = set(stopwords.words('english') + [\"i've\"])\n",
    "STOP_PUNCT = set('.\"\\'&”“’,:;/*()[]{}@#')\n",
    "\n",
    "def normalize_token(token):\n",
    "    for regex, marker in [(REGEX_URL, TOKEN_URL), (REGEX_NUM, TOKEN_NUM)]:\n",
    "        if re.match(regex, token):\n",
    "            return marker\n",
    "    return token\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def normalize_text(text, remove_stopwords=True, use_lemmatizer=False, remove_prefixes=True):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def is_stop_token(t):\n",
    "        return len(t) == 0 or t in STOP_PUNCT or remove_stopwords and t in STOPWORDS_EN\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # if the first token is a handler, it's normally irrelevant\n",
    "    if tokens[0].startswith('@'):\n",
    "        tokens.pop(0)\n",
    "\n",
    "    ntokens = []\n",
    "    \n",
    "    # STEP 1: cleanup, formatting\n",
    "    for token in tokens:\n",
    "        ntoken = normalize_token(token)\n",
    "        \n",
    "        if remove_prefixes and any(token.startswith(c) for c in ['@', '#']):\n",
    "            ntoken = ntoken[1:]\n",
    "        if not use_lemmatizer and is_stop_token(ntoken):\n",
    "            continue\n",
    "\n",
    "        ntokens.append(ntoken)\n",
    "        \n",
    "    if not use_lemmatizer:\n",
    "        return ' '.join(ntokens)\n",
    "    \n",
    "    ltokens = []\n",
    "    \n",
    "    # STEP 2: NLP tagging\n",
    "    pos_tags = nltk.pos_tag(ntokens)\n",
    "\n",
    "    # STEP 3: \"semantic\" cleaning\n",
    "    for token, pos in pos_tags:\n",
    "        if is_stop_token(token):\n",
    "            continue\n",
    "        if pos == 'CD':\n",
    "            ltoken = TOKEN_NUM\n",
    "        else:\n",
    "            ltoken = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "        ltokens.append(ltoken)\n",
    "           \n",
    "    return ' '.join(ltokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL: @VirginAmerica What @dhepburn said.\n",
      ">> dhepburn said\n",
      "\n",
      "POSITIVE: @VirginAmerica plus you've added commercials to the experience... tacky.\n",
      ">> plus added commercials experience ... tacky\n",
      "\n",
      "NEUTRAL: @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      ">> today ... must mean need take another trip !\n",
      "\n",
      "NEGATIVE: @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      ">> really aggressive blast obnoxious entertainment guests faces little recourse\n",
      "\n",
      "NEGATIVE: @VirginAmerica and it's a really big bad thing about it\n",
      ">> really big bad thing\n",
      "\n",
      "NEGATIVE: @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      ">> seriously would pay $ xxnum flight seats playing really bad thing flying va\n",
      "\n",
      "POSITIVE: @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\n",
      ">> yes nearly every time fly vx ear worm go away :)\n",
      "\n",
      "NEUTRAL: @VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n",
      ">> really missed prime opportunity men without hats parody xxurl\n",
      "\n",
      "POSITIVE: @virginamerica Well, I didn't…but NOW I DO! :-D\n",
      ">> well … ! :-D\n",
      "\n",
      "POSITIVE: @VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\n",
      ">> amazing arrived hour early good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_normalization(_df):\n",
    "    for i, row in _df.iterrows():\n",
    "        print(row.sentiment.upper() + \": \" + row.text + '\\n>> ' + normalize_text(row.text) + '\\n')\n",
    "\n",
    "show_normalization(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we add a new column to the dataset with the normalized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus added commercials experience ... tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>today ... must mean need take another trip !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>seriously would pay $ xxnum flight seats playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes nearly every time fly vx ear worm go away :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>really missed prime opportunity men without ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>positive</td>\n",
       "      <td>well … ! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazing arrived hour early good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica did you know that suicide is th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>know suicide second leading cause death among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>&lt;3 pretty graphics much better minimal iconogr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great deal ! already thinking xxnum trip austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>virginmedia i'm flying fabulous seductive skie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>negative</td>\n",
       "      <td>sfo-pdx schedule still mia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>excited first cross country flight lax mco hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n",
       "      <td>negative</td>\n",
       "      <td>flew nyc sfo last week fully sit seat due two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n",
       "      <td>positive</td>\n",
       "      <td>❤ ️ flying virginamerica ☺ ️ 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>know would amazingly awesome ? bos-fll please ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "0                 @VirginAmerica What @dhepburn said.   neutral   \n",
       "1   @VirginAmerica plus you've added commercials t...  positive   \n",
       "2   @VirginAmerica I didn't today... Must mean I n...   neutral   \n",
       "3   @VirginAmerica it's really aggressive to blast...  negative   \n",
       "4   @VirginAmerica and it's a really big bad thing...  negative   \n",
       "5   @VirginAmerica seriously would pay $30 a fligh...  negative   \n",
       "6   @VirginAmerica yes, nearly every time I fly VX...  positive   \n",
       "7   @VirginAmerica Really missed a prime opportuni...   neutral   \n",
       "8     @virginamerica Well, I didn't…but NOW I DO! :-D  positive   \n",
       "9   @VirginAmerica it was amazing, and arrived an ...  positive   \n",
       "10  @VirginAmerica did you know that suicide is th...   neutral   \n",
       "11  @VirginAmerica I &lt;3 pretty graphics. so muc...  positive   \n",
       "12  @VirginAmerica This is such a great deal! Alre...  positive   \n",
       "13  @VirginAmerica @virginmedia I'm flying your #f...  positive   \n",
       "14                             @VirginAmerica Thanks!  positive   \n",
       "15      @VirginAmerica SFO-PDX schedule is still MIA.  negative   \n",
       "16  @VirginAmerica So excited for my first cross c...  positive   \n",
       "17  @VirginAmerica  I flew from NYC to SFO last we...  negative   \n",
       "18                    I ❤️ flying @VirginAmerica. ☺️👍  positive   \n",
       "19  @VirginAmerica you know what would be amazingl...  positive   \n",
       "\n",
       "                                            norm_text  \n",
       "0                                       dhepburn said  \n",
       "1         plus added commercials experience ... tacky  \n",
       "2        today ... must mean need take another trip !  \n",
       "3   really aggressive blast obnoxious entertainmen...  \n",
       "4                                really big bad thing  \n",
       "5   seriously would pay $ xxnum flight seats playi...  \n",
       "6    yes nearly every time fly vx ear worm go away :)  \n",
       "7   really missed prime opportunity men without ha...  \n",
       "8                                        well … ! :-D  \n",
       "9                     amazing arrived hour early good  \n",
       "10  know suicide second leading cause death among ...  \n",
       "11  <3 pretty graphics much better minimal iconogr...  \n",
       "12  great deal ! already thinking xxnum trip austr...  \n",
       "13  virginmedia i'm flying fabulous seductive skie...  \n",
       "14                                           thanks !  \n",
       "15                         sfo-pdx schedule still mia  \n",
       "16  excited first cross country flight lax mco hea...  \n",
       "17  flew nyc sfo last week fully sit seat due two ...  \n",
       "18                     ❤ ️ flying virginamerica ☺ ️ 👍  \n",
       "19  know would amazingly awesome ? bos-fll please ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply normalization to ALL text\n",
    "df['norm_text'] = df.text.apply(normalize_text)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset with only pos/neg: (11541, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus added commercials experience ... tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>seriously would pay $ xxnum flight seats playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes nearly every time fly vx ear worm go away :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "1  @VirginAmerica plus you've added commercials t...  positive   \n",
       "3  @VirginAmerica it's really aggressive to blast...  negative   \n",
       "4  @VirginAmerica and it's a really big bad thing...  negative   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...  negative   \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...  positive   \n",
       "\n",
       "                                           norm_text  \n",
       "1        plus added commercials experience ... tacky  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  \n",
       "5  seriously would pay $ xxnum flight seats playi...  \n",
       "6   yes nearly every time fly vx ear worm go away :)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary = df[df.sentiment != 'neutral']\n",
    "print(\"Size of the dataset with only pos/neg: \" + str(df_binary.shape))\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training of all the examples, we will split 10% of the data for the validation (test) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 10386; test size: 1155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(df_binary['norm_text'], df_binary['sentiment'], test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"Training size: %d; test size: %d\" % (len(text_train), len(text_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preparation for smaller representation of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "\n",
    "y_train = [LABELS[l] for l in label_train]\n",
    "y_test = [LABELS[l] for l in label_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the performance of all the classifiers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key : (train score, test score)\n",
    "results = {}\n",
    "\n",
    "def add_result(*args):\n",
    "    results[args[0]] = args[1:]\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier\n",
    "\n",
    "As an absolute baseline, let's check the accuracy of a classifier that just outputs random predictions based on the training set's class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('random_with_class_dist', 0.6746581937223185, 0.6588744588744588)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='stratified', random_state=42)\n",
    "clf.fit(text_train, y_train)\n",
    "\n",
    "score_train = clf.score(text_train, y_train)\n",
    "score_test = clf.score(text_test, y_test)\n",
    "\n",
    "add_result('random_with_class_dist', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomian Naive Bayes using counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first approach, we use directly the word counts as features, and feed them into a multinomial Naive Bayes classifier from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "X_train = counter.fit_transform(text_train)\n",
    "X_test = counter.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with word counts as features (num_samples, num_features): (10386, 10201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the training set with word counts as features (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_multi_counts', 0.9433853264009243, 0.9038961038961039)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_multi_counts', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB using TF-IDF, direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with TF-IDF vectorization (num_samples, num_features): (10386, 9981)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "v_train = vectorizer.fit_transform(text_train)\n",
    "v_test = vectorizer.transform(text_test)\n",
    "\n",
    "X_train = v_train.toarray()\n",
    "X_test = v_test.toarray()\n",
    "\n",
    "print(\"Size of the training set with TF-IDF vectorization (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_gaussian_tf_idf', 0.8290968611592529, 0.6874458874458874)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_gaussian_tf_idf', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB using TF-IDF and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set with TF-IDF vectorization and feature selection (num_samples, num_features): (10386, 998)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(v_train, y_train)\n",
    "vs_train = selector.transform(v_train)\n",
    "vs_test  = selector.transform(v_test)\n",
    "\n",
    "X_train = vs_train.toarray()\n",
    "X_test = vs_test.toarray()\n",
    "\n",
    "print(\"Size of the training set with TF-IDF vectorization and feature selection (num_samples, num_features): \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb_gaussian_tf_idf_feature_sel', 0.865106874638937, 0.8112554112554112)\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('nb_gaussian_tf_idf_feature_sel', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we try different values of C and kernel\n",
    "parameters = {\n",
    "    'kernel': ('linear', 'rbf'),\n",
    "    'C': [1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "svc = SVC(gamma='auto') # setting gamma=auto to avoid warning\n",
    "clf = GridSearchCV(svc, parameters, cv=3, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the best and put it in the results\n",
    "score_train = clf.cv_results_['mean_train_score'][clf.best_index_]\n",
    "score_test = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "print(\"SVC would use these best parameters: %s, train score=%f, test score=%f\" % (clf.best_params_, score_train, score_test))\n",
    "add_result('SVC_%s_%.1f' % (clf.best_params_['kernel'], clf.best_params_['C']), score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_df = pd.DataFrame(clf.cv_results_)[['param_kernel', 'param_C', 'mean_train_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time']]\n",
    "svc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=40)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "\n",
    "add_result('dtree', score_train, score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([(k, *v) for k, v in results.items()], columns=['method', 'score_train', 'score_test'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
